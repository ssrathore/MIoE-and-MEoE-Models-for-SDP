{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as sfs\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer\n",
    "from pyclustering.cluster.xmeans import xmeans, splitting_type\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from ensemble_models import adaBoost\n",
    "from ensemble_models import bagging\n",
    "from implicitModel import Implicit_ME\n",
    "from explicitModel import Explicit_ME\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecisionTree(X_train,y_train,X_test,y_test):\n",
    "    clf = DecisionTreeClassifier(criterion = \"entropy\", splitter = \"random\").fit(X_train,y_train)\n",
    "    y_pred_test = clf.predict(X_test)\n",
    "    \n",
    "    #matrices\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_test)\n",
    "    auc = metrics.roc_auc_score(y_test,  y_pred_test)\n",
    "    result = {\n",
    "      \"Accuracy\": metrics.accuracy_score(y_test, y_pred_test),\n",
    "      \"F1_score\": f1_score(y_test, y_pred_test, average='macro'),\n",
    "      \"AUC_score\": auc,\n",
    "      \"Prediction\": y_pred_test,\n",
    "      \"MCC\" : metrics.matthews_corrcoef(y_test,  y_pred_test)\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM(X_train,y_train,X_test,y_test):\n",
    "    clf = svm.SVC().fit(X_train,y_train)\n",
    "    y_pred_test = clf.predict(X_test)\n",
    "    \n",
    "    #matrices\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_test)\n",
    "    auc = metrics.roc_auc_score(y_test,  y_pred_test)\n",
    "    result = {\n",
    "      \"Accuracy\": metrics.accuracy_score(y_test, y_pred_test),\n",
    "      \"F1_score\": f1_score(y_test, y_pred_test, average='macro'),\n",
    "      \"AUC_score\": auc,\n",
    "      \"Prediction\": y_pred_test,\n",
    "      \"MCC\" : metrics.matthews_corrcoef(y_test,  y_pred_test)\n",
    "      \n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN(X_train,y_train,X_test,y_test):\n",
    "    clf = KNeighborsClassifier(n_neighbors=2).fit(X_train,y_train)\n",
    "    y_pred_test = clf.predict(X_test)\n",
    "    \n",
    "    #matrices\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_test)\n",
    "    auc = metrics.roc_auc_score(y_test,  y_pred_test)\n",
    "    result = {\n",
    "      \"Accuracy\": metrics.accuracy_score(y_test, y_pred_test),\n",
    "      \"F1_score\": f1_score(y_test, y_pred_test, average='macro'),\n",
    "      \"AUC_score\": auc,\n",
    "      \"Prediction\": y_pred_test,\n",
    "      \"MCC\" : metrics.matthews_corrcoef(y_test,  y_pred_test)\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Logistic(X_train,y_train,X_test,y_test):\n",
    "    clf = LogisticRegression(max_iter=100,penalty = 'none').fit(X_train,y_train)\n",
    "    y_pred_test = clf.predict(X_test)\n",
    "    \n",
    "    #matrices\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_test)\n",
    "    auc = metrics.roc_auc_score(y_test,  y_pred_test)\n",
    "    result = {\n",
    "      \"Accuracy\": metrics.accuracy_score(y_test, y_pred_test),\n",
    "      \"F1_score\": f1_score(y_test, y_pred_test, average='macro'),\n",
    "      \"AUC_score\": auc,\n",
    "      \"Prediction\": y_pred_test,\n",
    "      \"MCC\" : metrics.matthews_corrcoef(y_test,  y_pred_test)\n",
    "      \n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gate_ntw(X_train,y_train,X_test,y_test):\n",
    "    gateDT = DecisionTreeClassifier(criterion = \"entropy\", splitter = \"best\").fit(X_train,y_train)\n",
    "    gateSVM = svm.SVC(probability=True).fit(X_train,y_train)\n",
    "    gateKNN = KNeighborsClassifier(n_neighbors=2).fit(X_train,y_train)\n",
    "    gateLOG = LogisticRegression(max_iter=10000).fit(X_train,y_train)\n",
    "    voting_clf = VotingClassifier(\n",
    "    estimators=[('DecisionTree',gateDT), ('SVM',gateSVM),('KNN',gateKNN),('Logistic',gateLOG)],voting='soft')\n",
    "    voting_clf.fit(X_train, y_train)\n",
    "    final_predictions = voting_clf.predict(X_test)\n",
    "    result = {\n",
    "      \"Accuracy\": metrics.accuracy_score(y_test, final_predictions),\n",
    "      \"F1_score\": f1_score(y_test, final_predictions, average='macro'),\n",
    "      \"AUC_score\": metrics.roc_auc_score(y_test,  final_predictions),\n",
    "      \"Prediction\": final_predictions,\n",
    "      \"MCC\" : metrics.matthews_corrcoef(y_test,  final_predictions)\n",
    "    }\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(data,name):\n",
    "    \n",
    "    #Data Preprocessing\n",
    "    df = pd.DataFrame(data[0])\n",
    "    X= df.iloc[ : , :-1].values\n",
    "    y=[]\n",
    "    if \"AEEEM\" in name:\n",
    "        y = df['class'].str.decode(\"utf-8\").map({'buggy': 1, 'clean': 0})\n",
    "    elif \"JIRA\" in name:\n",
    "        y= df['RealBugCount'].apply(lambda x : 1 if(x > 0) else 0)\n",
    "    else:\n",
    "        y= df['defects'].apply(lambda x : 1 if(x > 0) else 0)\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "    st_x= StandardScaler()  \n",
    "    X_train= st_x.fit_transform(X_train) \n",
    "    X_test= st_x.transform(X_test) \n",
    "    sel_ = SelectFromModel(LogisticRegression(C=1, penalty='l1', solver='liblinear'))\n",
    "    sel_.fit(X_train, y_train)\n",
    "    sel_.get_support()\n",
    "    X_train = sel_.transform(X_train)\n",
    "    X_test = sel_.transform(X_test)\n",
    "    oversample = SMOTE()\n",
    "    X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
    "    \n",
    "    #Models\n",
    "    experts = [DecisionTree,KNN,SVM,Logistic]\n",
    "    impME = Implicit_ME(X_train,y_train,X_test,y_test,5,experts,gate_ntw)\n",
    "    expME = Explicit_ME(X_train,y_train,X_test,y_test,experts,gate_ntw)\n",
    "    dt = DecisionTree(X_train,y_train,X_test,y_test)\n",
    "    svm = SVM(X_train,y_train,X_test,y_test)\n",
    "    knn = KNN(X_train,y_train,X_test,y_test)\n",
    "    log = Logistic(X_train,y_train,X_test,y_test)\n",
    "    bag = bagging(X_train,y_train,X_test,y_test)\n",
    "    ada = adaBoost(X_train,y_train,X_test,y_test)\n",
    "    \n",
    "    #results\n",
    "    acc = [dt[\"Accuracy\"],svm[\"Accuracy\"],knn[\"Accuracy\"],log[\"Accuracy\"],bag[\"Accuracy\"],ada[\"Accuracy\"],impME[\"Accuracy\"],expME[\"Accuracy\"]]\n",
    "    f1 = [dt[\"F1_score\"],svm[\"F1_score\"],knn[\"F1_score\"],log[\"F1_score\"],bag[\"F1_score\"],ada[\"F1_score\"],impME[\"F1_score\"],expME[\"F1_score\"]]\n",
    "    auc = [dt[\"AUC_score\"],svm[\"AUC_score\"],knn[\"AUC_score\"],log[\"AUC_score\"],bag[\"AUC_score\"],ada[\"AUC_score\"],impME[\"AUC_score\"],expME[\"AUC_score\"]]\n",
    "    mcc = [dt[\"MCC\"],svm[\"MCC\"],knn[\"MCC\"],log[\"MCC\"],bag[\"MCC\"],ada[\"MCC\"],impME[\"MCC\"],expME[\"MCC\"]]\n",
    "    res = {\n",
    "        \"acc\" : acc,\n",
    "        \"f1\" : f1,\n",
    "        \"auc\" : auc,\n",
    "        \"mcc\" : mcc\n",
    "    }\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution 1 done\n",
      "Execution 2 done\n",
      "Execution 3 done\n",
      "Execution 4 done\n",
      "Execution 5 done\n",
      "Execution 6 done\n",
      "Execution 7 done\n",
      "Execution 8 done\n",
      "Execution 9 done\n",
      "Execution 10 done\n"
     ]
    }
   ],
   "source": [
    "datasets = []\n",
    "\n",
    "datasets.append({\"path\": '../dataSet/AEEEM/EQ.arff', \"name\" : 'AEEEM-EQ' })\n",
    "datasets.append({\"path\": '../dataSet/AEEEM/JDT.arff', \"name\" : 'AEEEM-JDT' })\n",
    "datasets.append({\"path\": '../dataSet/AEEEM/Lucene.arff', \"name\" : 'AEEEM-LUCENE' })\n",
    "datasets.append({\"path\": '../dataSet/AEEEM/Mylyn.arff', \"name\" : 'AEEEM-MYLYN' })\n",
    "datasets.append({\"path\": '../dataSet/AEEEM/PDE.arff', \"name\" : 'AEEEM-PDE' })\n",
    "\n",
    "datasets.append({\"path\": '../dataSet/JIRA/activemq-5.0.0.arff', \"name\" : 'JIRA-ACTIVEMQ' })\n",
    "datasets.append({\"path\": '../dataSet/JIRA/derby-10.5.1.1.arff', \"name\" : 'JIRA-DERBY' })\n",
    "datasets.append({\"path\": '../dataSet/JIRA/groovy-1_6_BETA_1.arff', \"name\" : 'JIRA-GROOVY' })\n",
    "datasets.append({\"path\": '../dataSet/JIRA/hbase-0.94.0.arff', \"name\" : 'JIRA-HBASE' })\n",
    "datasets.append({\"path\": '../dataSet/JIRA/hive-0.9.0.arff', \"name\" : 'JIRA-HIVE' })\n",
    "datasets.append({\"path\": '../dataSet/JIRA/jruby-1.1.arff', \"name\" : 'JIRA-JRUBY' })\n",
    "datasets.append({\"path\": '../dataSet/JIRA/wicket-1.3.0-beta2.arff', \"name\" : 'JIRA-WICKET' })\n",
    "\n",
    "datasets.append({\"path\": '../dataSet/PROMISE/ant-1.7.arff', \"name\" : 'PROMISE-ANT' })\n",
    "datasets.append({\"path\": '../dataSet/PROMISE/camel-1.4.arff', \"name\" : 'PROMISE-CAMEL' })\n",
    "datasets.append({\"path\": '../dataSet/PROMISE/ivy-2.0.arff', \"name\" : 'PROMISE-IVY' })\n",
    "datasets.append({\"path\": '../dataSet/PROMISE/jedit-4.0.arff', \"name\" : 'PROMISE-JEDIT' })\n",
    "datasets.append({\"path\": '../dataSet/PROMISE/log4j-1.0.arff', \"name\" : 'PROMISE-LOG' })\n",
    "datasets.append({\"path\": '../dataSet/PROMISE/poi-2.0.arff', \"name\" : 'PROMISE-POI' })\n",
    "datasets.append({\"path\": '../dataSet/PROMISE/tomcat.arff', \"name\" : 'PROMISE-TOMCAT' })\n",
    "datasets.append({\"path\": '../dataSet/PROMISE/velocity-1.6.arff', \"name\" : 'PROMISE-VELOCITY' })\n",
    "datasets.append({\"path\": '../dataSet/PROMISE/xalan-2.4.arff', \"name\" : 'PROMISE-XALAN' })\n",
    "datasets.append({\"path\": '../dataSet/PROMISE/xerces-1.3.arff', \"name\" : 'PROMISE-XERCES' })\n",
    "\n",
    "acc_all = [] # 3d matrix iteration*dataset*models\n",
    "f1_all = []\n",
    "auc_all = []\n",
    "mcc_all = []\n",
    "for i in range(10):\n",
    "    acc = [] # 2d matrix dataset * models\n",
    "    f1 = []\n",
    "    auc = []\n",
    "    mcc = []\n",
    "    for singledata in datasets:\n",
    "        data = arff.loadarff(singledata[\"path\"])\n",
    "        res = main(data,singledata[\"name\"])    \n",
    "        acc.append(res[\"acc\"])\n",
    "        f1.append(res[\"f1\"])\n",
    "        auc.append(res[\"auc\"])\n",
    "        mcc.append(res[\"mcc\"])\n",
    "        \n",
    "    acc_all.append(acc)\n",
    "    f1_all.append(f1)\n",
    "    auc_all.append(auc)\n",
    "    mcc_all.append(mcc)\n",
    "    \n",
    "    DF_acc = pd.DataFrame(acc)  #results for each iteration\n",
    "    DF_f1 = pd.DataFrame(f1)\n",
    "    DF_auc = pd.DataFrame(auc)\n",
    "    DF_mcc = pd.DataFrame(mcc)\n",
    "    DF_acc.to_csv(\"NewResults/accuracy\"+str(i+1)+\".csv\")\n",
    "    DF_f1.to_csv(\"NewResults/f1_score\"+str(i+1)+\".csv\")\n",
    "    DF_auc.to_csv(\"NewResults/auc_score\"+str(i+1)+\".csv\")\n",
    "    DF_mcc.to_csv(\"NewResults/mcc_score\"+str(i+1)+\".csv\")\n",
    "    print(\"Execution \"+str(i+1) +\" done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution done\n"
     ]
    }
   ],
   "source": [
    "# calculating mean values from 10 iterations\n",
    "# x = iterations = 10\n",
    "# y = dataset = 22\n",
    "# z = models = 8\n",
    "mean_acc = [] #2d matrix dataset*model\n",
    "mean_f1 = []\n",
    "mean_auc = []\n",
    "mean_mcc = []\n",
    "\n",
    "for y in range(22):\n",
    "    acc = [] # 1d models avg\n",
    "    f1 = []\n",
    "    auc = []\n",
    "    mcc = []\n",
    "    for z in range(8):\n",
    "        temp_acc = []\n",
    "        temp_f1 = []\n",
    "        temp_auc = []\n",
    "        temp_mcc = []\n",
    "        for x in range(10):\n",
    "            temp_acc.append(acc_all[x][y][z])\n",
    "            temp_f1.append(f1_all[x][y][z])\n",
    "            temp_auc.append(auc_all[x][y][z])\n",
    "            temp_mcc.append(mcc_all[x][y][z])\n",
    "        avg_acc = np.mean(temp_acc)\n",
    "        avg_f1 = np.mean(temp_f1)\n",
    "        avg_auc = np.mean(temp_auc)\n",
    "        avg_mcc = np.mean(temp_mcc)\n",
    "        acc.append(avg_acc)\n",
    "        f1.append(avg_f1)\n",
    "        auc.append(avg_auc)\n",
    "        mcc.append(avg_mcc)\n",
    "    mean_acc.append(acc)\n",
    "    mean_f1.append(f1)\n",
    "    mean_auc.append(auc)\n",
    "    mean_mcc.append(mcc)\n",
    "        \n",
    "\n",
    "DF_mean_acc = pd.DataFrame(mean_acc)\n",
    "DF_mean_f1 = pd.DataFrame(mean_f1)\n",
    "DF_mean_auc = pd.DataFrame(mean_auc)\n",
    "DF_mean_mcc = pd.DataFrame(mean_mcc)\n",
    "DF_mean_acc.to_csv(\"NewResults/mean_accuracy.csv\")\n",
    "DF_mean_f1.to_csv(\"NewResults/mean_f1_score.csv\")\n",
    "DF_mean_auc.to_csv(\"NewResults/mean_auc_score.csv\")\n",
    "DF_mean_mcc.to_csv(\"NewResults/mean_mcc_score.csv\")\n",
    "print(\"Execution done\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
